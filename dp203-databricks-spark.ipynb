{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore data in a dataframe\n",
    "In this notebook, you'll use Spark in Azure Databricks to explore data in files. One of the core ways in which you work with data in Spark is to load data into a **Dataframe** object, and then query, filter, and manipulate the dataframe to explore the data it contains.\n",
    "## Ingest data\n",
    "Use the **&#9656; Run Cell** menu option at the top-right of the following cell to run it and download data files into the Databricks file system (DBFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "rm -r /dbfs/data\n",
    "mkdir /dbfs/data\n",
    "wget -O /dbfs/data/2019.csv https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/24/data/2019.csv\n",
    "wget -O /dbfs/data/2020.csv https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/24/data/2020.csv\n",
    "wget -O /dbfs/data/2021.csv https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/24/data/2021.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query data in files\n",
    "\n",
    "The previous cell downloaded three comma-separated values (CSV) files to the **data** folder in the DBFS storage for your workspace.\n",
    "\n",
    "Run the following cell to load the data from the file and view the first 100 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df = spark.read.load('data/*.csv', format='csv')\n",
    "display(df.limit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the file relates to sales orders, but doesn't include the column headers or information about the data types. To make more sense of the data, you can define a *schema* for the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "orderSchema = StructType([\n",
    "    StructField(\"SalesOrderNumber\", StringType()),\n",
    "    StructField(\"SalesOrderLineNumber\", IntegerType()),\n",
    "    StructField(\"OrderDate\", DateType()),\n",
    "    StructField(\"CustomerName\", StringType()),\n",
    "    StructField(\"Email\", StringType()),\n",
    "    StructField(\"Item\",StringType()),\n",
    "    StructField(\"Quantity\",IntegerType()),\n",
    "    StructField(\"UnitPrice\", FloatType()),\n",
    "    StructField(\"Tax\", FloatType())\n",
    "])\n",
    "\n",
    "df = spark.read.load('/data/*.csv', format='csv', schema=orderSchema)\n",
    "display(df.limit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the data includes the column headers.\n",
    "\n",
    "To verify that the appropriate data types have been defined, you van view the schema of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze data in a dataframe\n",
    "The dataframe object in Spark is similar to a *Pandas* dataframe in Python, and includes a wide range of functions that you can use to manipulate, filter, group, and otherwise analyze the data it contains.\n",
    "\n",
    "## Filter a dataframe\n",
    "\n",
    "Run the following cell to:\n",
    "\n",
    "- Filter the columns of the sales orders dataframe to include only the customer name and email address.\n",
    "- Count the total number of order records\n",
    "- Count the number of distinct customers\n",
    "- Display the customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customers = df['CustomerName', 'Email']\n",
    "print(customers.count())\n",
    "print(customers.distinct().count())\n",
    "display(customers.distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the following details:\n",
    "\n",
    "- When you perform an operation on a dataframe, the result is a new dataframe (in this case, a new customers dataframe is created by selecting a specific subset of columns from the df dataframe)\n",
    "- Dataframes provide functions such as count and distinct that can be used to summarize and filter the data they contain.\n",
    "- The `dataframe['Field1', 'Field2', ...]` syntax is a shorthand way of defining a subset of column. You can also use **select** method, so the first line of the code above could be written as `customers = df.select(\"CustomerName\", \"Email\")`\n",
    "\n",
    "Now let's apply a filter to include only the customers who have placed an order for a specific product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = df.select(\"CustomerName\", \"Email\").where(df['Item']=='Road-250 Red, 52')\n",
    "print(customers.count())\n",
    "print(customers.distinct().count())\n",
    "display(customers.distinct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can “chain” multiple functions together so that the output of one function becomes the input for the next - in this case, the dataframe created by the select method is the source dataframe for the where method that is used to apply filtering criteria.\n",
    "\n",
    "### Aggregate and group data in a dataframe\n",
    "\n",
    "Run the following cell to aggregate and group the order data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "productSales = df.select(\"Item\", \"Quantity\").groupBy(\"Item\").sum()\n",
    "display(productSales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the results show the sum of order quantities grouped by product. The **groupBy** method groups the rows by *Item*, and the subsequent **sum** aggregate function is applied to all of the remaining numeric columns (in this case, *Quantity*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlySales = df.select(year(\"OrderDate\").alias(\"Year\")).groupBy(\"Year\").count().orderBy(\"Year\")\n",
    "\n",
    "display(yearlySales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the results show the number of sales orders per year. Note that the select method includes a SQL **year** function to extract the year component of the *OrderDate* field, and then an **alias** method is used to assign a columm name to the extracted year value. The data is then grouped by the derived *Year* column and the **count** of rows in each group is calculated before finally the **orderBy** method is used to sort the resulting dataframe.\n",
    "\n",
    "## Query data using Spark SQL\n",
    "\n",
    "As you’ve seen, the native methods of the dataframe object enable you to query and analyze data quite effectively. However, many data analysts are more comfortable working with SQL syntax. Spark SQL is a SQL language API in Spark that you can use to run SQL statements, or even persist data in relational tables.\n",
    "\n",
    "### Use Spark SQL in PySpark code\n",
    "\n",
    "The default language in Azure Synapse Studio notebooks is *PySpark*, which is a Spark-based Python runtime. Within this runtime, you can use the **spark.sql** library to embed Spark SQL syntax within your Python code, and work with SQL constructs such as tables and views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"salesorders\")\n",
    "\n",
    "spark_df = spark.sql(\"SELECT * FROM salesorders\")\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that:\n",
    "\n",
    "- The code persists the data in the **df** dataframe as a temporary view named **salesorders**. Spark SQL supports the use of temporary views or persisted tables as sources for SQL queries.\n",
    "- The **spark.sql** method is then used to run a SQL query against the **salesorders** view.\n",
    "- The results of the query are stored in a dataframe.\n",
    "\n",
    "### Run SQL code in a cell\n",
    "\n",
    "While it’s useful to be able to embed SQL statements into a cell containing PySpark code, data analysts often just want to work directly in SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%sql\n",
    "\n",
    "SELECT YEAR(OrderDate) AS OrderYear,\n",
    "    SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue\n",
    "    FROM salesorders\n",
    "    GROUP BY YEAR(OrderDate)\n",
    "    ORDER BY OrderYear;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that:- The ``%sql` line at the beginning of the cell (called a magic) indicates that the Spark SQL language runtime should be used to run the code in this cell instead of PySpark.\n",
    "- The SQL code references the **salesorder** view that you created previously using PySpark.- The output from the SQL query is automatically displayed as the result under the cell.\n",
    "> **Note**: For more information about Spark SQL and dataframes, see the [Spark SQL documentation](https://spark.apache.org/docs/2.2.0/sql-programming-guide.html).\n",
    "\n",
    "## Visualize data with Spark\n",
    "\n",
    "A picture is proverbially worth a thousand words, and a chart is often better than a thousand rows of data. While notebooks in Azure Databricks include support for visualizing data from a dataframe or Spark SQL query, it is not designed for comprehensive charting. However, you can use Python graphics libraries like matplotlib and seaborn to create charts from data in dataframes.\n",
    "\n",
    "### View results as a visualization\n",
    "\n",
    "Run the following cell to query the **salesorders** table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%sqlSELECT * FROM salesorders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the table of results, select **+** and then select **Visualization** to view the visualization editor, and then apply the following options:\n",
    "    - **Visualization type**: Bar\n",
    "    - **X Column**: Item\n",
    "    - **Y Column**: *Add a new column and select* **Quantity**. *Apply the* **Sum** *aggregation*.\n",
    "    \n",
    "          Save the visualization and then re-run the code cell to view the resulting chart in the notebook.\n",
    "          \n",
    "### Get started with matplotlib\n",
    "    \n",
    "You can get mroe control over data visualizations by using graphics libraries.\n",
    "    \n",
    "Run the following cell to retrieve some sales order data into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery = \"SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\\n",
    "             SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \\\n",
    "             FROM salesorders \\\n",
    "             GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\\n",
    "             ORDER BY OrderYear\"\n",
    "\n",
    "\n",
    "df_spark = spark.sql(sqlQuery)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the data as a chart, we’ll start by using the matplotlib Python library. This library is the core plotting library on which many others are based, and provides a great deal of flexibility in creating charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# matplotlib requires a Pandas dataframe, not a Spark one\n",
    "df_sales = df_spark.toPandas()\n",
    "\n",
    "# Create a bar plot of revenue by year\n",
    "plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the results, which consist of a column chart with the total gross revenue for each year. Note the following features of the code used to produce this chart:\n",
    "\n",
    "- The **matplotlib** library requires a Pandas dataframe, so you need to convert the Spark dataframe returned by the Spark SQL query to this format.\n",
    "- At the core of the **matplotlib** library is the **pyplot** object. This is the foundation for most plotting functionality.\n",
    "- The default settings result in a usable chart, but there’s considerable scope to customize it, as you'll see by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the plot area\n",
    "plt.clf()\n",
    "\n",
    "# Create a bar plot of revenue by year\n",
    "plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')\n",
    "\n",
    "# Customize the chart\n",
    "plt.title('Revenue by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Revenue')\n",
    "plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot is technically contained with a **Figure**. In the previous examples, the figure was created implicitly for you; but you can create it explicitly.\n",
    "\n",
    "With # Create a Figure; fig = plt.figure(figsize=(8,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A figure can contain multiple subplots, each on its own axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the plot area\n",
    "plt.clf()\n",
    "# Create a figure for 2 subplots (1 row, 2 columns)\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10,4))\n",
    "\n",
    "# Create a bar plot of revenue by year on the first axis\n",
    "ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')\n",
    "ax[0].set_title('Revenue by Year')\n",
    "\n",
    "# Create a pie chart of yearly order counts on the second axis\n",
    "yearly_counts = df_sales['OrderYear'].value_counts()\n",
    "ax[1].pie(yearly_counts)\n",
    "ax[1].set_title('Orders per Year')\n",
    "ax[1].legend(yearly_counts.keys().tolist())\n",
    "\n",
    "# Add a title to the Figure\n",
    "fig.suptitle('Sales Data')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: To learn more about plotting with matplotlib, see the [matplotlib documentation](https://matplotlib.org/).\n",
    "\n",
    "### Use the seaborn library\n",
    "\n",
    "While **matplotlib** enables you to create complex charts of multiple types, it can require some complex code to achieve the best results. For this reason, over the years, many new libraries have been built on the base of matplotlib to abstract its complexity and enhance its capabilities. One such library is **seaborn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Clear the plot area\n",
    "plt.clf()\n",
    "\n",
    "# Create a bar chart\n",
    "ax = sns.barplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
